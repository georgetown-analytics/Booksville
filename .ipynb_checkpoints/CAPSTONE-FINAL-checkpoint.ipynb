{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necesssary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import psycopg2 as pg2\n",
    "import datetime as dt\n",
    "# package used for converting the data into datetime format\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## Execution time counter\n",
    "#%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish connection to the PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn= pg2.connect('dbname = Booksville user= postgres password =891630 host= 127.0.0.1')\n",
    "cur=conn.cursor()\n",
    "df_raw = pd.read_sql_query('select * from public.\"keepa\" limit 1000', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the dimension of the raw data to see if its properly imported\n",
    "print('Starting size of our Dataset ')\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out count of each datatype in the dataframe\n",
    "df_raw.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Aggregator\n",
    "\n",
    "Amazon's listing price for books is stored in one of the three columns in the sql dump we obtained from Keepa. If Amazon is fullfilling the sale, the price is stored in amazon_price column. But if the book is sold by a third party seller, the listing price would be marketplace_new_price for new books and marketplace_used_price for used books.\n",
    "\n",
    "We are combining the three columns in to one column called 'price' and assign its values based on the three given price assignment information.\n",
    "\n",
    "The aggregator function adds the new column to the dataset and assigns the value that appears first from the following list and finally drops the three columns from the dataset.\n",
    "\n",
    "     * amazon_Price        \n",
    "     * marketplace_new        \n",
    "     * marketplace_used_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PriceAggregator(original_df):\n",
    "    \n",
    "    df=original_df\n",
    "    # create a copy of the three columns to choose amazon price from\n",
    "    df_copy=df[['amazon_price','marketplace_new_price','marketplace_used_price']]\n",
    "    \n",
    "    # Replace missing price denoted by -1 to Null in all three price columns   \n",
    "    for item in df_copy:\n",
    "        df_copy[item].replace('-1',np.nan, inplace=True)\n",
    "        \n",
    "    # Add a new column to store the aggregated price with default value of 'amazon_price'         \n",
    "    df.insert(79,'price',df_copy['amazon_price'].astype('float'))\n",
    "    \n",
    "    \n",
    "    #Loop throgh all three columns to assign non-null value to the newly created price column. \n",
    "    #Keep amazon_price as it is if it is not null, otherwise assign marketplace_new_price as the new price. \n",
    "    #Where both 'amazon_price' and 'marketplace_new_price' are null, price will be set to \n",
    "    #'marketplace_used_price' regardless of its value.\n",
    "    \n",
    "    for i in range(df['price'].size):\n",
    "        if pd.isnull(df['price'][i]):\n",
    "            if pd.isnull(df_copy['marketplace_new_price'][i]):\n",
    "                if pd.isnull(df_copy['marketplace_used_price'][i]):\n",
    "                    pass\n",
    "                else:\n",
    "                    df['price'][i]=df_copy['marketplace_used_price'][i]\n",
    "            else:\n",
    "                df['price'][i]=df_copy['marketplace_new_price'][i]\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Delete records where price value is missing since that is what we are trying to predict \n",
    "    df.dropna(subset=['price'], axis=0, inplace=True)\n",
    "    \n",
    "    #Reset index after dropping rows with missing price\n",
    "    df.reset_index(drop= True, inplace=True)\n",
    "    \n",
    "    #Delete old price columns after assigning aggregated price to a brand new column    \n",
    "    df.drop(['amazon_price','marketplace_new_price','marketplace_used_price'], axis=1 , inplace=True)\n",
    "    \n",
    "    #Return the a dataframe with a new price column added to the original dataframe\n",
    "    return df   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=PriceAggregator(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete duplicate records, if there are any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data size before deleting duplicates\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data size after deleting duplicates\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from .shape() function there are 99600 unique records with 77 features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight of the dataframe\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include= 'all' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was stored in the PostgreSQL database as text(string) type and running discriptive statistics with .describe().The dataframe doesn't tell us much until we do the proper type of conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Wrangling \n",
    " \n",
    "Data wrangling is the process of converting data from the initial format to a format that may be better for analysis. As part of the wrangling process we are applying different techniques to come up with a cleaner and complete dataset to apply machine learning. Here are some of the steps we are following;\n",
    "    - Identify Missing Values\n",
    "    - Replace or Delete Missing Values\n",
    "    - Correct Data format\n",
    "    - Aggregate highly related categorical values where necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace missing values with Null Values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace every missing values with Numpy Null in order to keep uniformity and computational speed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(np.NaN)\n",
    "df.replace('', np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count and sort null values in every coulumn in descending order\n",
    "df.isna().sum().sort_values(ascending=False).to_frame(name='Count of Null Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null or missing value implies that we dont have information about the feature. We can delete the features that contain Null values for the majority of the records, because keeping them will not provide anything about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of features that contain null value for more than 50% of the records based on the the above observation\n",
    "Null_features=['coupon','offers','liveOffersOrder','promotions','buyBoxSellerIdHistory','features','upcList','variations',\n",
    "               'hazardousMaterialType','genre','platform','variationCSV','parentAsin','department','size','model','color'\n",
    "               ,'partNumber','mpn','brand','edition','format']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names with their number of null values\n",
    "df[Null_features].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "We can delete these features without losing any useful information from our data since more than 50% of the records in the above list contain null values. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete columns that contain very high count of null values \n",
    "df.drop(Null_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the dataset to confirm the features are dropped\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining Null values in our data where the total count is relatively small, we will replace them by a statistically representative values like mean or mode.\n",
    "* Mode value is for categorical columns where there is a clear majority of null values or will be replaced by 'Unknown'\n",
    "* Mean value is used for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigns column names that contain null values to a list\n",
    "with_Nulls=df.loc[:, df.isna().sum()!=0].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists down the number of null values in every column in descending order\n",
    "df[with_Nulls].isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what kind of information is in each column\n",
    "df[with_Nulls].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample shows that the records are mainly comprised of string or categorical values. Lets further divide the series based on the number of missing (Null) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Nulls2Unknown=['categoryTree_4','categoryTree_3','categoryTree_2','author','studio','publisher','manufacturer',\n",
    "              'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the highest frequency value(Mode) in the given list of features, it only shows the count not the value. \n",
    "#Based on the count we can tell if there's a statistical representative mode value to replace the nulls.  \n",
    "for item in with_Nulls:\n",
    "    print(f'{item}\\t\\t{df[item].value_counts().max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that our data contains 100,000 records we can clearly see the high mode value will replace the null values for some of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following 3 features have very high Mode value, therefore we'll replace nulls by mode\n",
    "Nulls2Mode=['languages_0','categoryTree_0','categoryTree_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = df.filter(['languages_0','categoryTree_0','categoryTree_1']).mode()\n",
    "df[Nulls2Mode]=df[Nulls2Mode].fillna(df.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       \n",
    "     \n",
    "               \n",
    "  \n",
    "  \n",
    "For the following features since there is no one single category with a high frequency(Mode) in the group, we are filling the missing(Null) values with 'Unknown'.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "NullswithNoMode=df.loc[:, df.isna().sum()!=0].columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the top 3 most frequent records in each column, it shows that there is no dominant value that can be used\n",
    "#as a mode to replace null values. Therefore, we are replacing null values with 'Unknown'.\n",
    "for item in NullswithNoMode:\n",
    "    print(item)\n",
    "    print(df[item].value_counts().nlargest(3))\n",
    "    print('Total Number of null values in %s = %d' % (item,df[item].isna().sum()))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace nulls with 'Unknown' for multimodel features\n",
    "df[NullswithNoMode]=df[NullswithNoMode].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are still missing or null values in the dataset\n",
    "df[df.loc[:, df.isna().sum()!=0].columns].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have entirely replaced the null and missing values in the dataset by statistically representative values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data imported from postgreSQL to pandas dataframe contain columns as object type(string). Most of those features are actually nemerical values, and we will convert the object data type in to the proper format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets group all those features that are in string (object) format and convert them to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert columns that contain numerical values to numeric data type using pandas to_numeric\n",
    "numeric=['availabilityAmazon',\n",
    "       'ean','hasReviews', 'isEligibleForSuperSaverShipping', 'isEligibleForTradeIn',\n",
    "       'isRedirectASIN', 'isSNS', 'lastPriceChange','lastRatingUpdate', 'lastUpdate', 'listedSince', \n",
    "       'newPriceIsMAP', 'numberOfItems','numberOfPages', 'offersSuccessful', 'packageHeight',\n",
    "       'packageLength', 'packageQuantity', 'packageWeight', 'packageWidth',\n",
    "       'publicationDate', 'releaseDate', 'rootCategory','stats_atIntervalStart', 'stats_avg', 'stats_avg30', 'stats_avg90',\n",
    "       'stats_avg180', 'stats_current', 'stats_outOfStockPercentage30',\n",
    "       'stats_outOfStockPercentage90', 'stats_outOfStockPercentageInInterval',\n",
    "       'trackingSince','sales_rank', 'price']\n",
    "#cols = ['productType','rootCategory','stats_atIntervalStart','availabilityAmazon','hasReviews','isRedirectASIN','isSNS','isEligibleForTradeIn','isEligibleForSuperSaverShipping', 'ean','hasReviews', 'availabilityAmazon','isEligibleForTradeIn','lastPriceChange','lastRatingUpdate','lastUpdate','lastRatingUpdate','lastUpdate','listedSince',\"newPriceIsMAP\", \"numberOfItems\", \"numberOfPages\",\"packageHeight\", \"packageLength\",\"packageQuantity\", \"packageWeight\", \"packageWidth\",'stats_avg', 'stats_avg30', 'stats_avg90', 'stats_avg180', 'stats_current',\"stats_outOfStockPercentage30\", \"stats_outOfStockPercentage90\",\"stats_outOfStockPercentageInInterval\",\"trackingSince\",'upc','price','amazon_price', 'marketplace_new_price', 'marketplace_used_price', 'sales_rank']\n",
    "df[numeric] = df[numeric].apply(pd.to_numeric, errors='coerce', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings=df.loc[:, df.dtypes == np.object].columns.tolist()\n",
    "print('\\n'+ 'Sample of the dataset with only categorical information'+'\\n')\n",
    "df[strings].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can delete 'asin', 'ean' and 'imageCSV' columns since the information they contain is not characteristic discription of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['asin','imagesCSV','ean', 'upc'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print features that are object type\n",
    "df.loc[:, df.dtypes == np.object].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "Information regarding what language a book is written in and whether its an original print or translated version is recorded in the 'language_0' column. These two information are separated by comma, this column can be split into two parts and stored separately in order to reduce categorical data that we have to encode later.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['languages_0'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = df['languages_0'].str.split(\",\", n = 1, expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language_1']=new[0]\n",
    "df['language_2']=new[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce categories from 9 to 6 groupes by combining related categories \n",
    "#df['language_1'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group English, english and Middle English to one categry\n",
    "df['language_1'].replace(('English', 'english','Middle English'),'English', inplace = True)\n",
    "\n",
    "#group Spanish,Portuguese and Latin under \"Spanish\"\n",
    "df['language_1'].replace(('Spanish', 'Portuguese','Latin'),'Spanish', inplace = True)\n",
    "\n",
    "#group Chinese, mandarin Chinese and simplified chinese under Chinese \n",
    "df['language_1'].replace(('Simplified Chinese', 'Mandarin Chinese','Chinese'),'Chinese', inplace = True)\n",
    "\n",
    "#group Arabic,Hebrew and Turkish under Middle Eastern\n",
    "df['language_1'].replace(('Arabic', 'Hebrew','Turkish'),'Middle Eastern', inplace = True)\n",
    "\n",
    "# group languages with single entry record in to one group called 'Others'\n",
    "df['language_1'].replace(('Hindi', 'Scots','Filipino','Malay','Dutch','Greek','Korean','Romanian','Czech'),'Others', inplace = True)\n",
    "\n",
    "\n",
    "#group Danish and Norwegian under 'Scandinavian'  \n",
    "df['language_1'].replace(('Danish', 'Norwegian'),'Scandinavian', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace ('published','Published,Dolby Digital 1.0','Published,DTS-HD 5.1') by Published\n",
    "df['language_2'].replace(('published','Published,Dolby Digital 1.0','Published,DTS-HD 5.1'),'Published', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['language_1','language_2']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we have copied the information into new columns we can delete the languages_0 column\n",
    "df.drop(['languages_0'], axis=1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning binding column\n",
    "df.binding.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binding column contains 73 differnt categories that are mostly related and some of them contain very small elements. We will aggregate closely related categories to reduce the dimension of our variables to avoid curse of dimensionality \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.binding.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of identical items to create a group an aggregated category \n",
    "dict={'Unknown':['Printed Access Code', 'Unknown','Health and Beauty', 'Lawn & Patio', 'Workbook', 'Kitchen', 'Automotive', 'Jewelry'],\n",
    "     'spiral':[ 'Spiral-bound', 'Staple Bound', 'Ring-bound', 'Plastic Comb', 'Loose Leaf', 'Thread Bound'],\n",
    "     'magazines':[ 'Journal', 'Single Issue Magazine', 'Print Magazine'],\n",
    "     'audios':[ 'Audible Audiobook', 'Audio CD', 'DVD', 'Album', 'MP3 CD', 'Audio CD Library Binding'],\n",
    "     'digital_prints':[ 'CD-ROM', 'Blu-ray', 'DVD-ROM', 'Kindle Edition', 'Video Game', 'Sheet music', 'Software Download',\n",
    " 'Personal Computers', 'Electronics', 'Game', 'Wireless Phone Accessory'],\n",
    "     'hardcovers':['Hardcover', 'Hardcover-spiral', 'Turtleback', 'Roughcut'],\n",
    "     'others':[ 'Cards', 'Pamphlet', 'Calendar', 'Map', 'Stationery', 'Accessory', 'Misc. Supplies', 'Office Product', 'Poster',\n",
    " 'Wall Chart', 'Bookmark', 'JP Oversized'],\n",
    "     'paperbacks':[ 'Paperback', 'Perfect Paperback', 'Mass Market Paperback', 'Flexibound', 'Print on Demand (Paperback)',\n",
    " 'Comic', 'Puzzle', 'Paperback Bunko'],\n",
    "     'leather_bonded':[ 'Bonded Leather', 'Leather Bound', 'Imitation Leather', 'Vinyl Bound'],\n",
    "     'board_book':[ 'Board book', 'Baby Product', 'Toy', 'Rag Book', 'Card Book', 'Bath Book', 'Pocket Book'],\n",
    "     'schoolLibrary_binding':[ 'School & Library Binding', 'Library Binding', 'Textbook Binding']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in dict.items():\n",
    "    df.binding.replace(val,key, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.binding.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#catTree_under10.categoryTree_2.values= 'Other'\n",
    "def groupUnder10(x):\n",
    "    cond = df[x].value_counts()\n",
    "    threshold = 10\n",
    "    df[x] = np.where(df[x].isin(cond.index[cond > threshold ]), df[x], 'Others')\n",
    "    print('All the different categories that contain less than 10 items in the %s column are grouped together and renamed to \"Others\".' %x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['categoryTree_1','categoryTree_2','categoryTree_3','categoryTree_4']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupUnder10('categoryTree_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group under 10 counts in to one for categoryTree_3 column\n",
    "groupUnder10('categoryTree_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupUnder10('categoryTree_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['categoryTree_0','categoryTree_1','categoryTree_2','categoryTree_3','categoryTree_4']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some features are duplicated within the dataset, lets delete those duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete duplicated features\n",
    "\n",
    "duplicates=df[['label', 'manufacturer', 'publisher', 'studio']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].equals(df['manufacturer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].equals(duplicates['publisher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].equals(duplicates['studio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(['label', 'manufacturer', 'publisher', 'studio'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=['label', 'manufacturer', 'publisher', 'studio'],keep='first').value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above 4 columns contain 89493 duplicated informartion out of 99600 total records we can keep one of those and drop the reamining ones without losing useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep publisher and drop the rest\n",
    "df.drop(['label', 'manufacturer','studio'], axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we decide whether to use standard deviation or interquntile range to identify outliers, lets plot the data points using a distribution plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distWithBox(data):\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    sns.set(style=\"ticks\")\n",
    "\n",
    "    x = df[data]\n",
    "\n",
    "    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, \n",
    "                                        gridspec_kw={\"height_ratios\": (.15, .85)})\n",
    "\n",
    "    sns.boxplot(x, ax=ax_box)\n",
    "    sns.distplot(x, ax=ax_hist)\n",
    "\n",
    "    ax_box.set(yticks=[])\n",
    "    sns.despine(ax=ax_hist)\n",
    "    sns.despine(ax=ax_box, left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution and box plot of the raw data with outliers\n",
    "distWithBox('price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normally distributed data, the skewness should be about 0. A skewness value > 0 means that there is more weight in the left tail of the distribution. The function skewtest can be used to determine if the skewness value is close enough to 0, statistically speaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "from scipy.stats import skewtest\n",
    "skew(df['price'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x=df['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the the distribution plot, the skewtest and the box plot that price is not normally distributed. The price  data is right skewed and there are outlier values that need to be handled.\n",
    "\n",
    "When a data set has outliers or extreme values, we summarize a typical value using the median as opposed to the mean.  When a data set has outliers, variability is often summarized by a statistic called the interquartile range, which is the difference between the first and third quartiles. The first quartile, denoted Q1, is the value in the data set that holds 25% of the values below it. The third quartile, denoted Q3, is the value in the data set that holds 25% of the values above it. The quartiles can be determined following the same approach that we used to determine the median, but we now consider each half of the data set separately. The interquartile range is defined as follows:\n",
    "\n",
    "Interquartile Range(IQR) = Q3-Q1\n",
    "\n",
    "Outliers are values  1.5*IQR below Q1 or above Q3 or equivalently, values below Q1-1.5 IQR or above Q3+1.5 IQR.\n",
    "These are referred to as Tukey fences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import percentile\n",
    "data=df['price']\n",
    "q25, q75 = percentile(data, 25), percentile(data, 75)\n",
    "iqr = q75 - q25\n",
    "print('Percentiles:\\n\\t25th=%.3f \\n\\t75th=%.3f \\n\\tIQR=%.3f' % (q25, q75, iqr))\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 1.5\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "print('Identified outliers: %d' % len(outliers) )\n",
    "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
    "print('Non-outlier observations: %d' % len(outliers_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers=[] \n",
    "data_1=df['price'] \n",
    "for item in data_1:\n",
    "    if item <lower or item>upper:\n",
    "        outliers.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df['price']\n",
    "outlier_indices=list(data_1.index[(x<lower) | (x> upper)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outlier_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(axis=0,index=outlier_indices, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets plot distribution with and box plot to see the change after we trim down the outliers\n",
    "distWithBox('price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Between Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running pearson correlation between numeric valued features to see if there is any linear dependence between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor=df.corr()\n",
    "sns.heatmap(cor,cmap=\"PiYG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots will show us the distribution of categorical data against a continuous variable. We are using a boxplot to visualize the distribution of values in binding and language columns against price. \n",
    "Based on the visualization we can see that there is not so much overlap in the binding category, which implies that it is a good predictor of price. But when it comes to language_1, books in almost in every language fall within a price range of 500 to 2000( which is $5 to $20). It implies that knowing what language a book is written in doesn't tell us how much it would worth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['price'],y=df['binding'], data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=df['language_1'],x=df['price'], data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=['author','language_1','language_2','binding','categoryTree_0', 'categoryTree_1', 'categoryTree_2', 'categoryTree_3',\n",
    "       'categoryTree_4','productGroup','publisher','title','type','language_1','language_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in cat_cols:\n",
    "    df[item]=df[item].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cat_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding to convert string to representative numeric values\n",
    "df[cat_cols]= df[cat_cols].apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 records from the dataset to check if all the records are converted to numbers\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples.\n",
    "\n",
    "threshold .8 * (1 - .8)\n",
    "\n",
    "Using 0.8 as a threshhold, we will remove features with less than 20 percent variation within itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X=df.loc[:, df.columns != 'price']\n",
    "df_y=df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "print('%s Number of features before VarianceThreshhold'%len(df_X.columns))\n",
    "\n",
    "selector=VarianceThreshold(threshold=(.8*(1-.8)))\n",
    "FeaturesTransformed=selector.fit_transform(df_X)\n",
    "\n",
    "## print the support and shape of the transformed features\n",
    "print(selector.get_support())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_X[df_X.columns[selector.get_support(indices=True)]]\n",
    "cols=data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced=pd.DataFrame(FeaturesTransformed, columns=cols)\n",
    "df_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_reduced\n",
    "target=df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yellowbrick for Feature Selection\n",
    "\n",
    "we are using yellowbrick's feature selection method for finding and selecting the most useful features and eliminate zero importance features from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Features for Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using yellowbrick feature selection method with random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "viz = FeatureImportances(RandomForestRegressor(), ax=ax)\n",
    "viz.fit(data, target)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(viz.feature_importances_,\n",
    "                                   index=data.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## important features for Random Forest Regression\n",
    "RF_importants=feature_importances.index[feature_importances.importance!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[RF_importants].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting price using random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "X=df[RF_importants]\n",
    "Y=df['price']\n",
    "model=RandomForestRegressor()\n",
    "X_train, X_test, Y_train, Y_test= split(X,Y,test_size=0.25, random_state=42)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_test=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1=sns.distplot(target,hist=False, color='r',label=\"Actual price\")\n",
    "sns.distplot(Y_test,hist=False,color='b', label=\"Predicted price\", ax=ax1)\n",
    "plt.title(\" Actual Vs Predicted Price \")\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Proportion of Books')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Features for Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "viz = FeatureImportances(GradientBoostingRegressor(), ax=ax)\n",
    "viz.fit(data, target)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(viz.feature_importances_,\n",
    "                                   index=data.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## important features for gradient boosting regression\n",
    "GBR_importants=feature_importances.index[feature_importances.importance!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[GBR_importants].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "X=df[GBR_importants]\n",
    "Y=df['price']\n",
    "model=GradientBoostingRegressor()\n",
    "X_train, X_test, Y_train, Y_test= split(X,Y,test_size=0.25, random_state=42)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_test=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1=sns.distplot(target,hist=False, color='r',label=\"Actual price\")\n",
    "sns.distplot(Y_test,hist=False,color='b', label=\"Predicted price\", ax=ax1)\n",
    "plt.title(\" Actual Vs Predicted Price \")\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Proportion of Books')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Features for Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "viz = FeatureImportances( DecisionTreeRegressor(), ax=ax)\n",
    "viz.fit(data, target)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(viz.feature_importances_,\n",
    "                                   index=data.columns,\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## important features for decision tree regression\n",
    "DTR_importants=feature_importances.index[feature_importances.importance!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[DTR_importants].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "X=df[DTR_importants]\n",
    "Y=df['price']\n",
    "model=DecisionTreeRegressor()\n",
    "X_train, X_test, Y_train, Y_test= split(X,Y,test_size=0.25, random_state=42)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_test=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1=sns.distplot(target,hist=False, color='r',label=\"Actual price\")\n",
    "sns.distplot(Y_test,hist=False,color='b', label=\"Predicted price\", ax=ax1)\n",
    "plt.title(\" Actual Vs Predicted Price \")\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Proportion of Books')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "In this section we will implement several models that will predict price using the dependent variables and compare the accuracy, r-score, goodness of fit and plot residuals. Based on the scores and visual comparison of the plots, we will refine the best performing models using grid search to fine tune the hyperparameters to generate a better predictive model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function applies multiple models on the data and returns model name with r2-score and mean squared error value\n",
    "def ModelScores(data,target):\n",
    "    X=data\n",
    "    Y=target\n",
    "    \n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import math\n",
    "    from sklearn.model_selection import train_test_split as split\n",
    "    X_train, X_test, Y_train, Y_test= split(X,Y,test_size=0.25, random_state=42)\n",
    "    \n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.linear_model import RidgeCV\n",
    "    from sklearn.linear_model import LassoLars\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    from sklearn.linear_model import BayesianRidge\n",
    "    from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "    models={'Gradient Boost': GradientBoostingRegressor(),\n",
    "            'Random Forest': RandomForestRegressor(),\n",
    "            'Decision Tree': DecisionTreeRegressor(),\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'MLP': MLPRegressor(),\n",
    "            'Ridge CV': RidgeCV(),\n",
    "            'LassoLars':LassoLars(),\n",
    "            'Lasso':Lasso(),\n",
    "            'Elastic Search': ElasticNet(),\n",
    "            'Bayesian Ridge':BayesianRidge(),\n",
    "            'Ransac':RANSACRegressor()      \n",
    "           }\n",
    "    for name,model in models.items():\n",
    "        mdl=model\n",
    "        mdl.fit(X_train, Y_train)\n",
    "        prediction = mdl.predict(X_test)\n",
    "        print(name)\n",
    "        print(\"Accuracy Score\", r2_score(Y_test, prediction))\n",
    "        mse3 = mean_squared_error(Y_test, prediction)\n",
    "        print(\"The root mean square value\", math.sqrt(mse3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data\n",
    "target=df['price']\n",
    "ModelScores(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressors = {\n",
    "    \"Gradient Boost\": GradientBoostingRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor()\n",
    "    \n",
    "}\n",
    "\n",
    "for _, regressor in regressors.items():\n",
    "    visualizer = ResidualsPlot(regressor)\n",
    "    visualizer.fit(X_train, Y_train)\n",
    "    visualizer.score(X_test, Y_test)\n",
    "    visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.target import FeatureCorrelation\n",
    "feature_names = np.array(df.columns)\n",
    "data=df.loc[:, df.columns != 'price']\n",
    "target=df['price']\n",
    "figsize=(20, 20)\n",
    "visualizer = FeatureCorrelation(labels=feature_names)\n",
    "visualizer.fit(data, target)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation curve for decision tree regression and Random forest regression models\n",
    "\n",
    "from yellowbrick.model_selection import ValidationCurve\n",
    "\n",
    "# Extract the instances and target\n",
    "X = df_X\n",
    "y = df_y\n",
    "\n",
    "regressors = {\n",
    "        \"Gradient Boost\": GradientBoostingRegressor(),\n",
    "        \"Decision Tree\": DecisionTreeRegressor(),\n",
    "        \"Random Forest\": RandomForestRegressor()\n",
    "\n",
    "}\n",
    "for _, regressor in regressors.items():\n",
    "    viz = ValidationCurve(\n",
    "        regressor, param_name=\"max_depth\",\n",
    "        param_range=np.arange(1, 11), cv=10, scoring=\"r2\"\n",
    "    )\n",
    "    # Fit and poof the visualizer\n",
    "    viz.fit(X, y)\n",
    "    viz.poof()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the validation curve from the above three figures to narrow down the optimal 'max_depth' value range to use, for hyperparameter tuning in a  grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Score for Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from yellowbrick.model_selection import CVScores\n",
    "\n",
    "ind=df[RF_importants].values\n",
    "dep=df['price'].values\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "cv = KFold(10)\n",
    "oz = CVScores(\n",
    "    RandomForestRegressor(), ax=ax, cv=cv, scoring='r2'\n",
    ")\n",
    "\n",
    "oz.fit(ind, dep)\n",
    "oz.poof()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV score for Gradiet Boosting Regresor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import CVScores\n",
    "\n",
    "ind=df[GBR_importants].values\n",
    "dep=df['price'].values\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
    "oz = CVScores(\n",
    "    GradientBoostingRegressor(), ax=ax, cv=cv, scoring = 'r2'\n",
    ")\n",
    "\n",
    "oz.fit(ind, dep)\n",
    "oz.poof()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV score for Decision Tree Regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from yellowbrick.model_selection import CVScores\n",
    "\n",
    "ind=df[DTR_importants].values\n",
    "dep=df['price'].values\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "cv = KFold(10)\n",
    "oz = CVScores(\n",
    "    DecisionTreeRegressor(), ax=ax, cv=cv, scoring = 'r2'\n",
    ")\n",
    "\n",
    "oz.fit(ind, dep)\n",
    "oz.poof()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter tunung for decision tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "DecisionTree = DecisionTreeRegressor(random_state = 40)\n",
    "\n",
    "min_samples_split = [2,3,4,5,6,7,8]\n",
    "min_samples_leaf = [1,2,3,4,5]\n",
    "max_depth = [4,5,6,7,8,9]\n",
    "tuned_params = [{'min_samples_split': min_samples_split}, {'min_samples_leaf': min_samples_leaf},{'max_depth': max_depth}]\n",
    "n_folds = 5\n",
    "\n",
    "X=df[DTR_importants]\n",
    "Y=df['price']\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    DecisionTree, tuned_params, cv=n_folds\n",
    ")\n",
    "\n",
    "grid.fit(X, Y)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "GradientBoosting = GradientBoostingRegressor(random_state = 40)\n",
    "alphas = [0.001, 0.01, 0.1, 0.5, 0.9]\n",
    "sample_split = [2,3,4,5,6,7,8]\n",
    "max_depth = [4,5,6,7,8,9]\n",
    "learning_rate = [0.1, 0.3, 0.5, 0.7]\n",
    "tuned_params = [{'alpha': alphas}, {'min_samples_split': sample_split}, {'max_depth': max_depth}, {'learning_rate':learning_rate}]\n",
    "n_folds = 5\n",
    "\n",
    "X=df[GBR_importants]\n",
    "Y=df['price']\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    GradientBoosting, tuned_params, cv=n_folds\n",
    ")\n",
    "\n",
    "grid.fit(X, Y)\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RandomForest = RandomForestRegressor(random_state = 40)\n",
    "\n",
    "estimators = [10,50,100]\n",
    "sample_split = [2,3,4,5,6,7,8]\n",
    "sample_leaf = [1,2,3,4,5]\n",
    "max_depth = [4,5,6,7,8,9]\n",
    "tuned_params = [{'n_estimators': estimators}, {'min_samples_split': sample_split}, {'min_samples_leaf': sample_leaf},{'max_leaf_nodes': max_depth}]\n",
    "n_folds = 5\n",
    "\n",
    "X=df[RF_importants]\n",
    "Y=df['price']\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    RandomForest, tuned_params, cv=n_folds\n",
    ")\n",
    "\n",
    "grid.fit(X, Y)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "In this part we are using the result that we obtained from the grid search as an input to retrain our models. The grid search is applied with cross validation by taking the average score over 5 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[DTR_importants]\n",
    "Y=df['price']\n",
    "X_train, X_test, Y_train, Y_test= split(X,Y,test_size=0.25, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model=DecisionTreeRegressor(criterion='mse', max_depth=9, max_features=None,\n",
    "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=1,\n",
    "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "           presort=False, random_state=40, splitter='best')\n",
    "\n",
    "model.fit(X_train,Y_train)\n",
    "prediction13 = model.predict(X_test)\n",
    "print(\"Accuracy Score\", r2_score(Y_test, prediction13))\n",
    "mse = mean_squared_error(Y_test, prediction13)\n",
    "print(\"The root mean square value\", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[GBR_importants]\n",
    "Y=df['price']\n",
    "X_train, X_test, Y_train, Y_test= split(X,Y,test_size=0.25, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "             learning_rate=0.1, loss='ls', max_depth=5, max_features=None,\n",
    "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "             min_impurity_split=None, min_samples_leaf=1,\n",
    "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "             n_estimators=100, n_iter_no_change=None, presort='auto',\n",
    "             random_state=40, subsample=1.0, tol=0.0001,\n",
    "             validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "model.fit(X_train,Y_train)\n",
    "prediction13 = model.predict(X_test)\n",
    "print(\"Accuracy Score\", r2_score(Y_test, prediction13))\n",
    "mse = mean_squared_error(Y_test, prediction13)\n",
    "print(\"The root mean square value\", math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[RF_importants]\n",
    "Y=df['price']\n",
    "X_train, X_test, Y_train, Y_test= split(X,Y,test_size=0.25, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfg = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "           oob_score=False, random_state=40, verbose=0, warm_start=False)\n",
    "rfg.fit(X_train, Y_train)\n",
    "prediction14 = rfg.predict(X_test)\n",
    "print(\"Accuracy Score\", r2_score(Y_test, prediction14))\n",
    "mse2 = mean_squared_error(Y_test, prediction14)\n",
    "print(\"The root mean square value\", math.sqrt(mse2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_summarizingdata/bs704_summarizingdata7.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
